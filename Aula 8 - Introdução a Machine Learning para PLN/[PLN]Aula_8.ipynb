{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHvEggQV8Glz+3QO+DjtjO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrunaMoraisMenezesDSM/fatec_PLN_Codes/blob/master/Aula%208%20-%20Introdu%C3%A7%C3%A3o%20a%20Machine%20Learning%20para%20PLN/%5BPLN%5DAula_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aula 08 - Introdução a Machine Learning para PLN\n",
        "\n",
        "## Exemplo 01 - Aplicação do modelo de Naives em um texto"
      ],
      "metadata": {
        "id": "U7WLag1RhTf5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNrfEplwK2Lx",
        "outputId": "1389cb3e-0ced-415e-c0f6-87ea6003e4e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['eu', 'amo', 'pln'], 'positivo'), (['eu', 'odeio', 'bugs'], 'negativo'), (['amo', 'resolver', 'problemas'], 'positivo'), (['odeio', 'erros'], 'negativo'), (['bugs', 'são', 'legais'], 'positivo'), (['resolver', 'dificuldades', 'é', 'vida'], 'positivo'), (['eu', 'não', 'gosto', 'de', 'falhas'], 'negativo')]\n",
            "Texto: \"Eu amo resolver bugs\"\n",
            "Classe prevista: \"positivo\"\n",
            "Probabilidades:\n",
            "positivo\"0.0014050562510367166\"\n",
            "negativo\"0.0006277901785714285\"\n"
          ]
        }
      ],
      "source": [
        "# Passo 1 - Criando o Corpus\n",
        "\n",
        "corpus = [\n",
        "    (\"Eu amo PLN\", \"positivo\"),\n",
        "    (\"Eu odeio bugs\", \"negativo\"),\n",
        "    (\"Amo resolver problemas\", \"positivo\"),\n",
        "    (\"Odeio erros\", \"negativo\"),\n",
        "    (\"Bugs são legais\", \"positivo\"),\n",
        "    (\"Resolver dificuldades é vida\", \"positivo\"),\n",
        "    (\"Eu não gosto de falhas\", \"negativo\"),\n",
        "]\n",
        "  # Atribuindo textos e a sua classificação, sendo positivo ou negativo\n",
        "\n",
        "# Passo 2 - Processando o Texto\n",
        "import re # Importando o módulo re para manipulação de strings com expressões regulares\n",
        "from collections import defaultdict, Counter # Importando defaultdict e Counter para contagem de palavras e classes\n",
        "\n",
        "# Função que processa o texto, convertendo para minúsculas e extraindo as palavras\n",
        "def preprocess_text(text):\n",
        "  return re.findall(r'\\b\\w+\\b', text.lower()) # Encontrando palavras no texto em minúsculo com uma expressão regular\n",
        "\n",
        "# Processando o corpus e associando cada texto à sua classe\n",
        "processed_corpus = [(preprocess_text(text), label) for text, label in corpus] # Aplicando o pré-processamento a cada texto\n",
        "print(processed_corpus) # Exibindo o corpus processado, mostrando as palavras e suas classes\n",
        "\n",
        "# Passo 3 - Calculando probabilidades\n",
        "class_counts = Counter() # Contagem de classes (positivo, negativo)\n",
        "word_counts = defaultdict(Counter) # Contagem das palavras para cada classe\n",
        "total_words = defaultdict(int) # Contagem total de palavras para cada classe\n",
        "\n",
        "# Contagem das palavras por classe\n",
        "for words, label in processed_corpus:\n",
        "  class_counts[label] += 1 # Conta a ocorrência de cada classe\n",
        "  for word in words:\n",
        "    word_counts[label][word] += 1 # Conta a ocorrência de cada palavra em sua classe\n",
        "    total_words[label] # Conta o total de palavras em cada classe\n",
        "\n",
        "# Calculando as probabilidades de uma classe ser escolhida\n",
        "total_examples = sum(class_counts.values()) # Total de textos\n",
        "prior_probabilities = {cls: count / total_examples for cls, count in class_counts.items()} # Probabilidade de cada classe\n",
        "  # prior_probabilities -> Dicionário que armazena a probabilidade a priori de cada classe\n",
        "  # count ->  Quantidade de vezes que a classe 'cls' aparece no corpus\n",
        "  # total_examples -> Número total de textos\n",
        "\n",
        "# Função que calcula a probabilidade condicional de uma palavra dada uma classe\n",
        "def conditional_probability(word, label, alpha=1):\n",
        "  # word -> Palavra que estamos verificando\n",
        "  # label -> Classe para a qual estamos calculando a probabilidade condicional\n",
        "  # alpha -> Parâmetro de suavização de Laplace. O valor padrão é 1\n",
        "\n",
        "  # Fórmula de Laplace para suavização de probabilidades\n",
        "  return (word_counts[label][word] + alpha) / (total_words[label] + alpha * len(word_counts[label])) # Fórmula de Laplace\n",
        "    # word_counts[label][word] -> Quantidade de vezes a palavra apareceu na classe 'label'\n",
        "    # total_words[label] -> Número total de palavras na classe 'label'\n",
        "    # alpha * len(word_counts[label]) -> Número total de possíveis palavras que podem aparecer na classe, multiplicado por 'alpha' para garantir que a probabilidade nunca seja zero\n",
        "\n",
        "# Passo 4 - Classificar um novo texto\n",
        "def predict(text):\n",
        "  words = preprocess_text(text) # Processando o texto de entrada\n",
        "  probabilities = {} # Armazena as probabilidades de cada classe\n",
        "\n",
        "# Calculando as probabilidades para cada classe\n",
        "  for label in class_counts.keys(): # Iterando sobre todas as classes presentes no conjunto de dados\n",
        "    probabilities[label] = prior_probabilities[label] # Inicializando com a probabilidade escolhida da classe\n",
        "    # A probabilidade a priori é a chance de um texto pertencer a uma determinada classe sem considerar as palavras específicas\n",
        "\n",
        "    for word in words: # Iterando sobre todas as palavras do texto processado\n",
        "      probabilities[label] *= conditional_probability(word, label) # Multiplicando a probabilidade condicional de cada palavra\n",
        "  return max(probabilities, key=probabilities.get), probabilities # Retornando a classe com a maior probabilidade\n",
        "  # key=probabilities.get ->  faz com que a função max escolha classe com o valor de probabilidade mais alto\n",
        "\n",
        "# Passo 5 - Teste com um novo texto\n",
        "novo_texto = \"Eu amo resolver bugs\" # Texto a ser classificado\n",
        "classe, probs = predict(novo_texto) # Antecipa a classe do novo texto\n",
        "\n",
        "# Exibindo o texto de entrada, classe prevista e probabilidades de cada classe\n",
        "print(f'Texto: \"{novo_texto}\"')\n",
        "print(f'Classe prevista: \"{classe}\"')\n",
        "print(f'Probabilidades:')\n",
        "for label, prob in probs.items(): # Iterando classes e exibindo suas probabilidades\n",
        "  print(f'{label}\"{prob}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo 02 - Classificação do SVM (Support Vector Machines)"
      ],
      "metadata": {
        "id": "AmqU9Q6ek4dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn # Instalando biblioteca é usada para aprendizado de máquina, a scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp5FLEF6k-ZY",
        "outputId": "c6acee7c-f5f4-48c3-d181-937111197dc6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo do SVM"
      ],
      "metadata": {
        "id": "1w5GZxe2T-Ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 1 - Importando as bibliotecas a serem utilizadas\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Importando o vetorizador TF-IDF para transformar texto em vetores\n",
        "from sklearn.svm import SVC # Importando o modelo Support Vector Classifier (SVM)\n",
        "from sklearn.model_selection import train_test_split # Importando a função para dividir o dataset em treino e teste\n",
        "from sklearn.metrics import classification_report # Importando a função para gerar relatório de avaliação\n",
        "\n",
        "# Passo 2 - Dados exemplo\n",
        "# Criando textos de exemplo\n",
        "corpus = [\n",
        "    \"Eu amo PLN\", \"Eu odeio bugs\", \"Eu amo resolver problemas\",\n",
        "    \"Odeio erros\", \"Amo programação\", \"Não gosto de falhas\"\n",
        "]\n",
        "classes = [\"positivo\", \"negativo\", \"positivo\", \"negativo\", \"positivo\", \"negativo\"] # Classes correspondentes aos textos acima\n",
        "\n",
        "# Passo 3 - Pré-processamento e vetorização\n",
        "vectorizer = TfidfVectorizer() # Criando o objeto que aplica a transformação TF-IDF\n",
        "X = vectorizer.fit_transform(corpus) # Convertendo o corpus em uma matriz esparsa de características\n",
        "  # X -> São os dados de entrada\n",
        "y = classes # Definindo as classes correspondentes\n",
        "  # y -> São os rótulos ou classes\n",
        "\n",
        "# Passo 4 - Dividindo dos dados e treinamento do modelo\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "  # O método divide aleatoriamente os dados em dois conjuntos\n",
        "    # X_train -> Conjunto de dados de treinamento\n",
        "    # X_test -> Conjunto de dados de teste\n",
        "    # y_train -> Labels de treinamento correspondentes aos dados em X_train\n",
        "    # y_test -> Labels de teste correspondentes aos dados em X_test\n",
        "  # test_size = 0.3 -> Define a proporção de dados que será alocada para o conjunto de teste\n",
        "  # random_state = 42 -> Define a semente do gerador de números aleatórios\n",
        "\n",
        "svm_model = SVC(kernel='linear') # Criando o modelo SVM com kernel linear\n",
        "svm_model.fit(X_train, y_train) # Treinando o modelo com os dados de treino\n",
        "\n",
        "# Passo 5 - Avaliação do modelo\n",
        "y_pred = svm_model.predict(X_test) # Fazendo previsões usando o modelo\n",
        "print(classification_report(y_test, y_pred)) # Exibindo o relatório de avaliação do modelo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdfehRobWD4V",
        "outputId": "edd1b2a6-599b-491e-ad9b-66cdd4d5f9e7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo       1.00      1.00      1.00         1\n",
            "    positivo       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "comparação dos dois modelos"
      ],
      "metadata": {
        "id": "fbp_P5cYT_kR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo 03 - Comparando os classificadores com Scikit-Learn"
      ],
      "metadata": {
        "id": "wxeAscggliRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Importando Bibliotecas\n",
        "import nltk # Importando o pacote NLTK para processamento de linguagem natural\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Importando o vetorizador TF-IDF\n",
        "from sklearn.model_selection import train_test_split # Importando a função para dividir o dataset\n",
        "from sklearn.naive_bayes import MultinomialNB # Importando o classificador Naive Bayes para multinomial\n",
        "from sklearn.svm import SVC # Importando o classificador Support Vector Machine (SVM)\n",
        "from sklearn.metrics import classification_report # Importando a função para relatório de avaliação\n",
        "\n",
        "# Baixando o dataset de exemplo\n",
        "nltk.download('movie_reviews') # Baixando o conjunto de dados \"movie_reviews\" do NLTK\n",
        "from nltk.corpus import movie_reviews # Importando o corpus de críticas de filmes do NLTK\n",
        "\n",
        "# 2. Preparando dos dados\n",
        "# Coletando textos e classes\n",
        "documents = [(\" \".join(movie_reviews.words(fileid)), category) # Criando uma lista com o texto e a categoria de cada crítica\n",
        "    for category in movie_reviews.categories() # Iterando sobre as categorias de crítica (positivo, negativo)\n",
        "    for fileid in movie_reviews.fileids(category)] # Iterando sobre os arquivos de cada categoria\n",
        "\n",
        "# Separando textos e rótulos\n",
        "texts, labels = zip(*documents) # Separando os textos e as classes em duas listas\n",
        "\n",
        "# Transformando rótulos (positivo/negativo) em 0 e 1\n",
        "from sklearn.preprocessing import LabelEncoder # Importando o codificador de rótulos\n",
        "label_encoder = LabelEncoder() # Criando o codificador\n",
        "labels = label_encoder.fit_transform(labels) # Transformando as classes em valores numéricos (0 ou 1)\n",
        "\n",
        " # Dividindo dados em treino e teste\n",
        "texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
        "  # O método divide aleatoriamente os dados em dois conjuntos\n",
        "    # X_train -> Conjunto de dados de treinamento\n",
        "    # X_test -> Conjunto de dados de teste\n",
        "    # y_train -> Labels de treinamento correspondentes aos dados em X_train\n",
        "    # y_test -> Labels de teste correspondentes aos dados em X_test\n",
        "  # test_size = 0.3 -> Define a proporção de dados que será alocada para o conjunto de teste\n",
        "  # random_state = 42 -> Define a semente do gerador de números aleatórios\n",
        "\n",
        "# 3. Representando do texto com TF-IDF\n",
        "# Criando o vetorizador TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Criando o vetor TF-IDF, limitando o número de palavras para as 5000 mais frequentes\n",
        "\n",
        "# Ajustando e transformando textos\n",
        "X_train = vectorizer.fit_transform(texts_train) # Ajustando o vetor e transforma os textos de treino\n",
        "X_test = vectorizer.transform(texts_test) # Transformando os textos de teste\n",
        "\n",
        "# 4. Treinar os modelos\n",
        "# Treinamento do Naive Bayes\n",
        "nb_model = MultinomialNB() # Criando o modelo Naive Bayes para dados multinomiais\n",
        "nb_model.fit(X_train, labels_train) # Treinando o modelo com os dados de treino\n",
        "\n",
        "# Predição\n",
        "nb_predictions = nb_model.predict(X_test) # Fazendo previsões usando o modelo Naive Bayes\n",
        "\n",
        "# Treinamento do SVM\n",
        "svm_model = SVC(kernel='linear') # Criando o modelo SVM com kernel linear\n",
        "svm_model.fit(X_train, labels_train) # Treinando o modelo SVM com os dados de treino\n",
        "\n",
        "  # Predição\n",
        "svm_predictions = svm_model.predict(X_test) # Fazendo previsões usando o modelo SVM\n",
        "\n",
        "# 5. Avaliação\n",
        "\n",
        "# Avaliação do Naive Bayes\n",
        "print(\"Naive Bayes Performance:\")\n",
        "print(classification_report(labels_test, nb_predictions, target_names=label_encoder.classes_))\n",
        "  # Exibindo as métricas de desempenho para o modelo Naive Bayes\n",
        "    # classification_report -> Gera uma tabela de métricas de avaliação para cada classe\n",
        "    # labels_test -> Classes verdadeiras de cada amostra no conjunto de teste\n",
        "    # nb_predictions -> Previsões feitas pelo modelo Naive Bayes para o conjunto de teste\n",
        "    # target_names=label_encoder.classes_ -> Nomes das classes para as quais as métricas de desempenho serão calculadas\n",
        "    # label_encoder.classes_ -> Rótulos codificados numericamente\n",
        "\n",
        "# Avaliação do SVM\n",
        "print(\"SVM Performance:\")\n",
        "print(classification_report(labels_test, svm_predictions, target_names=label_encoder.classes_))\n",
        "  # Exibindo as métricas de performance para o modelo SVM\n",
        "    # classification_report -> Gera uma tabela de métricas de avaliação para cada classe\n",
        "    # labels_test -> Classes verdadeiras de cada amostra no conjunto de teste\n",
        "    # svm_predictions -> Previsões feitas pelo modelo SVM para o conjunto de teste\n",
        "    # target_names=label_encoder.classes_ -> Nomes das classes para as quais as métricas de desempenho serão calculadas\n",
        "    # label_encoder.classes_ -> Rótulos codificados numericamente"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkPsOf5AWbEe",
        "outputId": "585ef654-d7dd-4c7b-955e-abb28260bc84"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.79      0.84      0.81       302\n",
            "         pos       0.82      0.77      0.80       298\n",
            "\n",
            "    accuracy                           0.80       600\n",
            "   macro avg       0.80      0.80      0.80       600\n",
            "weighted avg       0.80      0.80      0.80       600\n",
            "\n",
            "SVM Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.82      0.80      0.81       302\n",
            "         pos       0.81      0.82      0.81       298\n",
            "\n",
            "    accuracy                           0.81       600\n",
            "   macro avg       0.81      0.81      0.81       600\n",
            "weighted avg       0.81      0.81      0.81       600\n",
            "\n"
          ]
        }
      ]
    }
  ]
}